Brook@CV
[深度学习与计算机视觉： 深度学习必知基本概念以及链式求导](https://www.cnblogs.com/wangguchangqing/p/10191266.html)    

F((权值1 * 输入1 + 权值2 * 输入2 + ... ) + b)

常用的激活函数F有：    
1. sigmoid函数(S型生长曲线：S(x) = 1 / (1 + e^-x)，其对x的导数可以用自身表示：S'(x) = (e^-x) / (1 + e^-x)^2 = S(x)(1-S(x)))
2. ReLU 函数(线性整流函数 Rectified Linear Unit)，又称修正线性单元，通常意义下，线性整流函数指代数学中的斜坡函数，即f(x) = max(0,x)<ul><li>带泄露线性整流函数(Leaky ReLU)的梯度为一个常数λ∈(0,1)，而不是0。换言之：f(x) = x if x > 0, f(x) = λx if x <= 0(如果设定λ为一个可通过反向传播算法Backpropagation学习的变量，则成为参数线性整流Parametric ReLU)</li><li>带泄露随机线性整流(Randomized Leaky ReLU, RReLU)，是在带泄露线性整流函数的基础上，将函数梯度λ是一个取自连续性均匀分布U(l,u)概率模型的随机变量，即λ ~ U(l,u),l<u且l,u∈[0,1)</li><li>
噪声线性整流(Noisy ReLU)是修正线性单元在考虑高斯噪声的基础上进行改进的变种，对不定输入x，加上了一定程度的正态分布的不确定性，即f(x)=max(0,x+Y)，其中Y~N(0,σ(x))</li></ul>

神经网络主要受训的是进行权值矩阵的调整以及不同神经元被激活的组合。

成本函数（损失函数、目标函数等）：在训练用于判断神经网络的输出结果和真实结果的相近程度。成本函数的值越小，表示越接近真实值。常用的成本函数有：
* 均方误差L=1/2(∑y-y')，y'神经网络输出的预测值
* 交叉熵L=-(y㏑y'+(1-y)㏑(1-y'))

梯度下降：是进行求解函数局部最小值的优化方法。
